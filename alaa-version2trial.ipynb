{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Computer Vision Project</h1>\n",
    "<h2>Phase 1</h2>\n",
    "<h3>Team 3</h3>\n",
    "<ul><li>Anas Salah</li>\n",
    "<li> Alaa Hamdy </li>\n",
    "<li>Ahmed Amr </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries and packages used</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import imutils\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load training images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 33402 images\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "picsFolder_path = \"train/train/\"\n",
    "with open('digitStruct.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# import colored pictures\n",
    "for i in range(len(data)):\n",
    "    image = cv2.imread(picsFolder_path + data[i]['filename'])\n",
    "    images.append(image)\n",
    "    temp=[]\n",
    "    for j in range(len(data[i]['boxes'])):\n",
    "        temp.append(data[i]['boxes'][j]['label'])\n",
    "    temp = np.array(temp)\n",
    "    labels.append(temp)\n",
    "# for image in os.listdir(picsFolder_path):\n",
    "#     images.append(image)\n",
    "print(\"we have\",len(images),\"images\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resizing Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "resizedImages =[]\n",
    "for image in images[:10]:\n",
    "    resizedImages.append(cv2.resize(image,(100,75)))\n",
    "\n",
    "print(len(resizedImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Images Copy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "imagesCopy = []\n",
    "for resizedimage in resizedImages:\n",
    "    imagesCopy.append(resizedimage)\n",
    "\n",
    "\n",
    "print(len(imagesCopy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing background from images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "backgroundremovedImages=[]\n",
    "\n",
    "for resizedimage in resizedImages:\n",
    "\n",
    "    # convert image into grayscale\n",
    "    gray = cv2.cvtColor(resizedimage, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a background subtractor object\n",
    "    bgSubtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "    # Apply the background subtractor to the grayscale image\n",
    "    fgMask = bgSubtractor.apply(gray)\n",
    "\n",
    "    # Apply morphological transformations to remove noise and fill holes\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel)\n",
    "    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Invert the mask to obtain the foreground\n",
    "    fgMask = cv2.bitwise_not(fgMask)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    fgImg = cv2.bitwise_and(resizedimage, resizedimage, mask=fgMask)\n",
    "\n",
    "    # Display the result\n",
    "    backgroundremovedImages.append(fgImg)\n",
    "\n",
    "print(len(backgroundremovedImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting Original Images copy from BGR to Grayscale  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "greyImages = [] \n",
    "# Convert BGR to grayscale:\n",
    "for backgroundremovedimages in backgroundremovedImages:\n",
    "    greyImages.append(cv2.cvtColor(backgroundremovedimages, cv2.COLOR_BGR2GRAY)) \n",
    "\n",
    "\n",
    "print(len(greyImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sharpening Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sharpenedImages = []\n",
    "for greyimage in greyImages:\n",
    "\n",
    "    \n",
    "    # apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(greyimage, (3, 3), 0)\n",
    "\n",
    "    # apply Laplacian filter to extract edges\n",
    "    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n",
    "    laplacian = cv2.convertScaleAbs(laplacian)\n",
    "\n",
    "    # convert original image to grayscale\n",
    "    gray_resized = cv2.cvtColor(resizedimage, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # apply edge sharpening using the grayscale original image and the Laplacian edges\n",
    "    sharpened = cv2.convertScaleAbs(gray_resized - laplacian)\n",
    "    sharpenedImages.append(sharpened)\n",
    "\n",
    "print(len(sharpenedImages))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adaptive Thresholding on Grayscale Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Set the adaptive thresholding:\n",
    "windowSize = 31\n",
    "windowConstant = -1\n",
    "\n",
    "binaryImages = []\n",
    "\n",
    "\n",
    "# Apply the threshold:\n",
    "for greyimage in greyImages:\n",
    "    binaryImages.append(cv2.adaptiveThreshold(greyimage, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,\n",
    "                                        windowSize, windowConstant))\n",
    "    \n",
    "\n",
    "print(len(binaryImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform Connected Components on the Binary Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "filteredImages =[]\n",
    "\n",
    "# Perform Connected Components:\n",
    "for binaryimage in binaryImages:\n",
    "    componentsNumber, labeledImage, componentStats, componentCentroids = cv2.connectedComponentsWithStats(binaryimage,\n",
    "                                                                                                          connectivity=4)\n",
    "    # Set the minimum pixels for the area filter to filter connected components:\n",
    "    minArea = 20\n",
    "\n",
    "    # Get the indices/labels of the remaining components based on the area stat\n",
    "    # (skip the background component at index 0)\n",
    "    remainingComponentLabels = [i for i in range(1, componentsNumber) if componentStats[i][4] >= minArea]\n",
    "    # Filter the labeled pixels based on the remaining labels,\n",
    "    # assign pixel intensity to 255 (uint8) for the remaining pixels\n",
    "    # y3ni b3d de el mafood yfdal the largest connected components that hopefully contains the digits\n",
    "    # one problem is that background connected components may still exist\n",
    "    filteredImages.append(np.where(np.isin(labeledImage, remainingComponentLabels) == True, 255, 0).astype('uint8'))\n",
    "\n",
    "    \n",
    "print(len(filteredImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Morphological Operations - Closing on Filtered Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "closingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for filteredimage in filteredImages:\n",
    "    closingImages.append(cv2.morphologyEx(filteredimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))\n",
    "    \n",
    "\n",
    "print(len(closingImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Opening on the Closed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "openingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for closingimage in closingImages:\n",
    "    openingImages.append(cv2.morphologyEx(closingimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gaussian Blur on Opened Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothedImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "    smoothedImages.append(cv2.GaussianBlur(closingimage, (3, 3), 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Canny Edge Detection on Smoothed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "edgeImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for smoothedimage in smoothedImages:\n",
    "# perform canny edge detection:\n",
    "    edgeImages.append(cv2.Canny(closingimage, 100, 200))\n",
    "\n",
    "\n",
    "print(len(edgeImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Drawing bounding boxes on digits </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "# Get each bounding box\n",
    "# Find the big contours on the filtered image:\n",
    "for edgeimage in edgeImages:\n",
    "    contours, hierarchy = cv2.findContours(edgeimage, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_poly = [None] * len(contours)\n",
    "    # The Bounding Rectangles will be stored here:\n",
    "    boundRect = []\n",
    "    # Alright, just look for the outer bounding boxes:\n",
    "    for i, c in enumerate(contours):\n",
    "        if hierarchy[0][i][3] == -1:\n",
    "            contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "            boundRect.append(cv2.boundingRect(contours_poly[i]))\n",
    "       # Get the dimensions of the image\n",
    "    # height, width, channels = imagesCopy[index].shape\n",
    "\n",
    "    # Sort the bounding boxes based on their areas in descending order\n",
    "    # boundRect = sorted(boundRect, key=lambda x: x[2] * x[3], reverse=True)\n",
    "\n",
    "    # Create an empty list to store the selected bounding boxes\n",
    "    # selectedBoundRect = []\n",
    "\n",
    "    # Iterate through each bounding box\n",
    "    # for i, box in enumerate(boundRect):\n",
    "    #     # Check if the current bounding box overlaps with any of the previously selected bounding boxes\n",
    "    #     overlaps = False\n",
    "    #     for selectedBox in selectedBoundRect:\n",
    "    #         if box[0] < selectedBox[0] + selectedBox[2] and box[0] + box[2] > selectedBox[0] and \\\n",
    "    #                 box[1] < selectedBox[1] + selectedBox[3] and box[1] + box[3] > selectedBox[1]:\n",
    "    #             overlaps = True\n",
    "    #             break\n",
    "    #     # If the current bounding box doesn't overlap with any of the previously selected bounding boxes, add it to the list\n",
    "    #     if not overlaps:\n",
    "    #         selectedBoundRect.append(box)\n",
    "        # print(len(boundRect))\n",
    "    # Draw the bounding boxes on the (copied) input image:\n",
    "    for i in range(len(boundRect)):\n",
    "        color = (0, 255, 0)\n",
    "        # area= int(boundRect[i][2]) * int(boundRect[i][3])\n",
    "        # x, y, w, h = boundRect[i]\n",
    "\n",
    "        # # Calculate the center of the bounding box\n",
    "        # center_x = x + (w / 2)\n",
    "        # center_y = y + (h / 2)\n",
    "\n",
    "        # filter contours according to the area of the rectangle (parameter re5em)\n",
    "        if ( int(boundRect[i][2])*int(boundRect[i][3])>100 and int(boundRect[i][2])*int(boundRect[i][3])<1000):\n",
    "            cv2.rectangle(imagesCopy[index], (int(boundRect[i][0]), int(boundRect[i][1])),\n",
    "                          (int(boundRect[i][0] + boundRect[i][2]), int(boundRect[i][1] + boundRect[i][3])), color, 2)\n",
    "    index= index + 1\n",
    "\n",
    "print(len(edgeImages))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Show Images with bounding boxes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    # plt.imshow(images[i])\n",
    "    # plt.show()\n",
    "    cv2.imshow('image',imagesCopy[i])\n",
    "    cv2.waitKey(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
